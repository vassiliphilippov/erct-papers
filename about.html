<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="UTF-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="description" content="Understanding the ERCT Standard">
	<link rel="stylesheet" href="/assets/css/standard.css">
	<title>About</title>
    <link rel="icon" type="image/png" sizes="192x192" href="/assets/img/icons/favicon128.png">
</head>

<body>
	<div class="wrapper">

		<header class="header">
			<div class="header__container">
				<div class="header__logo header__logo--contain">
					<a aria-label="Go to Home Page" href="index.html">
						<img src="/assets/img/logo.svg" alt="company logo">
					</a>
				</div>
				<nav class="header__menu menu">
					<ul>
						<li><a href="index.html">Papers</a></li>
						<li><a href="standard.html">ERCT Standard</a></li>
						<li><a class="menu__active" href="about.html">About</a></li>
					</ul>
				</nav>
				<button type="button" aria-label="open/closed menu" class="header__icon icon-menu"><span></span></button>
			</div>
		</header>

		<main class="main">
			<section class="main__hero hero">
				<div class="hero__container">
					<div class="hero__body">
						<h1 class="hero__title title">About this website and ERCT Standard</h1>
					</div>
				</div>
			</section>
			<section class="main__introduction introduction">
				<div class="introduction__container">
					<div class="introduction__text">
<style>
    .introduction__text h2 {
        font-size: 24px;
        font-weight: 600;
        margin-top: 20px;
        margin-bottom: 0px;
    }
    .introduction__text h3 {
        font-size: 20px;
        font-weight: 600;
        margin-bottom: -10px;
    }
    .introduction__text a {
        text-decoration: underline;
    }
</style>
<h2>The Challenge of Educational Research</h2>

<p>
    Let's be honest: figuring out what <em>really</em> works in education is hard.
    Like, <em>really</em> hard.  It's not like testing a new lightbulb – you can't just
    flip a switch and see immediate, clear results.  We're dealing with complex human beings, 
    diverse classrooms, and a million interacting factors.  
</p>
<p>
    Think of it like trying to understand why one plant grows taller than another in a tangled, 
    overgrown garden – is it the soil? The sunlight?  
    The whispered encouragement you give it every morning? (Okay, maybe not that last one... but you get the idea.)
</p>

<h2>The Promise and Pitfalls of RCTs</h2>

<p>
    So, when I started digging deep into educational research, specifically Randomized Controlled Trials (RCTs), 
    I was looking for solid ground.  RCTs are supposed to be the "gold standard," the equivalent of a 
    meticulously controlled lab experiment in the messy world of education.  
    The idea is simple:  take two groups, randomly assign them to either get an intervention 
    (like a new teaching method) or not, and then compare the results.  Sounds perfect, right?
</p>

<p>
    Well... not always.  I kept encountering RCTs that, while technically following the "rules," felt... flimsy. 
    Maybe the intervention only lasted a week.  Maybe they only tested for improvement in one very specific skill.  
    Maybe the "control group" (the kids who didn't get the new thing) was barely described.  
    It was like reading a recipe that said, "Bake until done."  Helpful?  Not so much.
</p>

<h2>My   Need for a Practical Filter</h2>

<p>
    I needed a way to quickly assess whether a study was designed in a way that made its findings, 
    well, useful for my own research. Not necessarily "perfect" – because perfect is the enemy of good, 
    especially in education research – but robust enough to give me some confidence. 
    I wasn't looking to declare winners and losers, or to say whether an intervention "worked" in some absolute sense. 
    I just wanted to know: was the study designed in a way that minimized the most common pitfalls?
</p>

<h2>Introducing the ERCT Standard</h2>

<p>
    That's how the ERCT Standard was born.  It's essentially a personal checklist, a set of 12 criteria 
    organized into three levels, that I developed for myself.  Think of it as a series of "sanity checks."  
    Does the study last long enough to see real effects (Term/Year Duration)?  
    Did they randomize at the classroom or school level to avoid kids influencing each other (Class/School-level RCT)?  
    Did they look at the impact on <em>all</em> core subjects, not just the one they were focusing on (All-subject Exams)?
</p>

<p>
    You can find find more detailed in the <a href="/standard.html">ERCT Standard Specification</a>
    (<a href="/docs/erct_specification.md">LLM-friendly Markdown version</a>).
</p>

<h2>Important Considerations and Disclaimers</h2>

<p>
    I realized that this checklist might be helpful to others navigating the often-murky waters of educational research. 
    So, I decided to share it.  But I want to be <em>crystal clear</em> about a few things:
</p>

<h3>1. This is a humble tool.</h3>
<p>
    It's not the ultimate authority on RCT quality. It's just a set of criteria <em>I</em> found helpful, designed with LLM evaluation in mind.
</p>

<h3>2. It's not about judging researchers.</h3>
<p>
    Educational studies are incredibly difficult to conduct.  Researchers are often working with limited resources, 
    navigating complex school systems, and dealing with all the unpredictable variables that come with human learners.
    Often the researches know how to make the study better but have limited resources to do so.
    The studies evaluated here are valuable contributions, regardless of how they score on this particular standard.  
    If anything, these studies deserve <em>more</em> funding to allow for more rigorous designs.
</p>

<h3>3. It's specifically for educational RCTs.</h3>
<p>
    It doesn't apply to meta-analyses, qualitative studies, or research outside of education. 
    It also doesn't tell you if an intervention was "effective" – that's for you to decide based on the 
    study's findings and your own context.
</p>

<h3>4. Mistakes are possible.</h3>
<p>
    While I've put a lot of effort into ensuring the accuracy of the evaluations (detailed prompts, multiple LLM checks, 
    manual reviews, even using Deep Research mode), I'm human.  If you spot an error, or disagree with an assessment, 
    please let me know!  I'll be happy to review and correct it promptly.</p>


<h2>The ERCT Standard: A Lens, Not a Verdict</h2>

<p>
    Think of the ERCT Standard like a pair of glasses. 
    It might help you see the details of an RCT study a little more clearly. 
    But it won't magically give you perfect vision, and it certainly won't tell you what to <em>do</em> with what you see.  
    That part is up to you.
</p>

<h2>A Call for Better Educational Research</h2>

<p>
    Ultimately, I hope this standard, and the evaluations based on it, contribute to a broader conversation about how we 
    design, conduct, and interpret educational research.  It's a conversation we <em>need</em> to have, because, like 
    Semmelweis and his handwashing, sometimes the most impactful discoveries are the ones that challenge our assumptions 
    and force us to rethink what we thought we knew. 
    And remember, the goal is better, but it is always unachievable perfection, we should always remember it.
</p>

<p>
    Your,<br>
    Vassili Philippov
</p>
</div>
</div>
</section>
</main>

<footer class="footer">
<div class="footer__container">
    <div class="footer__body">
        <div class="footer__logo footer__logo--contain">
            <a aria-label="Go to Home Page" href="/">
                <img src="/assets/img/logo.svg" alt="company logo">
            </a>
        </div>
        <div class="footer__line footer__line--contain">
            Copyright © 2025 Vassili Philippov.
        </div>
    </div>
</div>
</footer>

</div>
<script src="/assets/js/script.js"></script>
</body>

</html>