Here is my project specification. 
Please convert it into detailed project with all necessary file and step by step instruction how to deply this project using my infrastructure described.

# Overview

This project hosts a list of educational/scientific papers evaluated according to the “ERCT” standard 
(with the 12 CETD-SAYB-GRIP criteria). 
It uses Django as the primary backend framework, and it stores its data in a PostgreSQL database.
The whole project is stored in GitHub repo.

# Table of Contents

1. Key Features
2. Repo Structure
3. Database
4. Python Styleguide
5. Website
6. ERCT Standard
7. Examples of Paper JSON
8. AWS Infrastructure
9. About Myself
10. About Python code
11. Using UV package manager
12. Existing files
13. Readme file
14. Instructions


# 1. Key Features

Key features include:
* A Paper model (with fields for metadata and the 12 ERCT-related criteria).
* A web-based interface (paper_list_view, paper_detail_view) that lists papers and provides a detail page for each.
* A JSON-based API that provides GET and POST endpoints for listing and creating Paper objects.
* The project is containerized using Docker and orchestrated via docker-compose.
* A pyproject.toml file for dependency management.
* A script to initiate the database with the initial 20 papers by loading their content from JSON files from the repo (from data/initial_paper_jsons folder).
* The website should be available on 80 port in production environment and as port 8080 in development. 
* The development environment should use a local database in a separate container. The production environment should RDS database and defined in .env file.
* All Django code is located in the app/ directory.
* The project should use all the best practices of Django projects.
* The website should provide 3 API end points:
 - GET /api/papers/ - Returns an array of all Paper objects with their full details (titles, criteria, etc.).
 - POST /api/papers/ - Accepts a JSON object to create a new Paper.
 - GET /api/papers/<slug:slug>/ - Returns the JSON representation of the paper with that slug.
* All secrets should be described in two files .env.dev and .env.prod that are not commited into repo. 
* Use Django Ninja API for API endpoints (I want to learn it).
* Use standard Django Admin for admin interface.
* Use two docker compose files for development and production (docker-compose.yml and docker-compose.prod.yml).
* The project uses mypy and ruff for type checking and linting.
* Following PEP 621 standard for pyproject.toml file without third-party vendors lock-in.
* Using uv as package manager ()


# 2. Repo structure

The repository is hosted at GitHub at 
github.com/vassiliphilippov/erctpaperswebsite.git

erctpaperswebsite/
├── .dockerignore
├── .gitignore
├── DEPLOYMENT.md
├── docker-compose.yml
├── docker-compose.prod.yml
├── Dockerfile
├── pyproject.toml
├── README.md
└── app
    ├── manage.py
    ├── mysite
    └── papers
└── data
    └── initial_paper_jsons

# 3. Database

The project should use PostgreSQL database. 
The development environment should use a local database in a separate container. 
The production environment should RDS database and defined in .env file.

# 4. Python styleguide 

All project code should follow the following Python styleguide. We use 3.12 Python version.

```
# Glite Python style guide

Treat this style guide as a living document explaining emergent conventions and the reasoning behind them.

- [Main principles](#main-principles)
- [General](#general)
- [Type hinting](#type-hinting)
  - [Satisfy mypy and use explicit types as assertions](#satisfy-mypy-and-use-explicit-types-as-assertions)
  - [Use exhaustive matches where applicable](#use-exhaustive-matches-where-applicable)
  - [Use `int | None` instead of `Optional[int]`](#use-int--none-instead-of-optionalint)
  - [Use the type system to encode and validate business logic constraints](#use-the-type-system-to-encode-and-validate-business-logic-constraints)
  - [Use `T_Whatever` format for meaningful type variables](#use-t_whatever-format-for-meaningful-type-variables)
- [Asserts](#asserts)
  - [Use `assert` for defensive coding and documentation](#use-assert-for-defensive-coding-and-documentation)
  - [Assume that `assert`s are always run](#assume-that-asserts-are-always-run)
  - [Use positive assertion messages](#use-positive-assertion-messages)
- [Imports](#imports)
  - [Use direct imports (with exceptions documented below)](#use-direct-imports-with-exceptions-documented-below)
  - [Use absolute imports](#use-absolute-imports)
- [Dataclasses/pydantic](#dataclassespydantic)
  - [Use `dataclass` by default when values need to be grouped together](#use-dataclass-by-default-when-values-need-to-be-grouped-together)
  - [Make `dataclass` frozen and slotted by default](#make-dataclass-frozen-and-slotted-by-default)
  - [Use `pydantic` only where validation or serialisation is required](#use-pydantic-only-where-validation-or-serialisation-is-required)
- [Django](#django)
  - [Add `created_at`/`changed_at` meta-fields to models unless there are reasons not to](#add-created_atchanged_at-meta-fields-to-models-unless-there-are-reasons-not-to)
- [Links](#links)



## Main principles:

* If something can be automated out of the style guide and human attention, it should be. Therefore, if a convention
can be enforced with `ruff`, it doesn't belong here.
* The style we're after is concise, pragmatic, semantic-rich, and readable. It's alright to have a learning
curve for new colleagues if it's beneficial in the long run.
* There is no need for grand migrations whenever the style guide changes or new guidance is introduced. Ship
new code in the new style and do best effort adaptation of the old code, but don't delay new guidance until
legacy code can be reworked.
* Strive for robustness and early loud failures.
* This is guidance, not hard-and-fast rules. The guidance can be ignored when required.



## General

### Use the most recent Python syntax and libraries (as long as they are supported by our tooling)

#### Why:

* Stay ahead of deprecation warnings
* New syntax is generally cleaner

#### Do:

* `list[int]`
* `from collections.abc import Iterable`
* [PEP695](https://peps.python.org/pep-0695/) style generics (`def func[T](a: T) -> T`)
* `Path("foo") / "bar"`

#### Don't:

* `List[int]`
* `from typing import Iterable`
* old-style generics (`T = TypeVar("T"); def func(a: T) -> T`)
* `os.path.join("foo", "bar")`




### Use `@property` only for simple operations

Never put non-trivial computation or IO inside a `@property`.

#### Why:

It's impossible to distinguish an innocent field access or a heavy/IO `@property` access on the call site,
which easily becomes a problem whenever the access is looped or requires tight timing (e.g. whilst
holding a database lock).

#### Do:

```python
class MyClass:
    a: int

    @property
    def aplusone(self):
        return self.a + 1

    def get_db_data(self):
        return fetch_b_from_db(self.a)
```

#### Don't:

```python
class MyClass:
    a: int

    @property
    def aplusone(self):
        return self.a + 1

    @property
    def db_data(self):
        return fetch_b_from_db(self.a)
```


### Use explicit checks instead of relying on values being falsey

Don't use the idiomatic falsiness of empty lists, zeroes, empty strings, and `None`s,
check them explicitly.

#### Why:

This idiom is too error-prone, especially in presence of `T | None` types.

#### Do:

```python
if x is None:
    ...
```

```python
if len(x) == 0:
    ...
```

```python
if (x := get_x()) is not None:
    ...
```

#### Don't:

Assuming non-`bool` x:

```python
if x:
    ...
```

```python
if x := get_x():
    ...
```


### Put more general, "context-y" function parameters first when defining functions

A useful rule of thumb is "would it be convenient to use `partial()` on this function".

#### Why:

Consistency, extra semantic information, and convenience of `partial()`.

#### Do:

```python
def foo(context, db, ids_to_fetch):
    ...
```

#### Don't:

```python
def foo(ids_to_fetch, context, db):
    ...
```


### Use keyword argument syntax when calling functions that have two or more heterogeneous parameters

#### Why:

Keyword arguments are more resilient to refactorings and typos and are easier to read.

#### Do:

```python
foo(
    db=db,
    user=current_user,
    is_registered=True,
)
```

Assuming that `obj1`, `obj2`, and `obj3` are homogenous (i.e. one type):

```python
foo(ctx, obj1, obj2, obj3)
```

#### Don't:

```python
foo(db, current_user, True)
```


### Write and store all durations in milliseconds

#### Why:

Primarily, consistency. We don't normally need to be more precise than milliseconds, and milliseconds
can always be coarsened up.

#### Do:

```python
QUERY_TIMEOUT: int = 5000  # milliseconds
```

#### Don't:

```python
QUERY_TIMEOUT: int = 5  # seconds
```


### Use "kind" instead of "type" in names

#### Why:

`type` clashes with built-in `type` too much.

#### Do:

```python
class UserKind(Enum):
    ...
```

#### Don't:

```python
class UserType(Enum):
    ...
```



## Type hinting

### Satisfy mypy and use explicit types as assertions

Prefer type inference and use explicit type annotations in three cases:

* wherever the types are required by `mypy` (function signatures, tricky inference, etc.)
* as assertions that the inferred type is what you think it is
* as documentation

#### Why

Relying on type inference makes things more concise, but sometimes the inferred type might not match
the intuition and even mask an error. Consider this example:

```python
a = foo()
b = foo()

print(str(a + b))
```

What's `a` and `b`? Imagine that when you wrote the code `foo` returned `int`, but then started returning `str`.
The snippet above will still typecheck, but output a completely unexpected value.

However, in most cases, something downstream will fail to type check, so use your judgment to decide if an explicit
type (effectively, an assertion that the type is inferred correctly) is worth it.

#### Do:

(when it makes sense)

```python
a: int = foo()
b = foo()
print(str(a + b))
```

#### Don't:

```python
def myfun() -> ...:
    a: int = 1
    b: int = 2
```


### Use exhaustive matches where applicable

Use `assert_never` to make mypy scream at you for forgetting to handle a branch or an element of a type union.

#### Why:

Types and code change over time. Whenever you have code like this

```python
X: TypeAlias = Foo | Bar

def f(x: X) -> None:
    if isinstance(x, Foo):
        ...
    else:
        ...
```

…it might subtly break when X becomes `Foo | Bar | Baz`. Instead, if you use

```python
X: TypeAlias = Foo | Bar

def f(x: X) -> None:
    if isinstance(x, Foo):
        ...
    elif isinstance(x, Bar):
        ...
    else:
        assert_never(x)
```

…mypy will complain that not every case is covered in the explicit branches above. You will know that
the code needs fixing before you even run the tests.

You can also use `assert_never` for unreachable code, e.g. when you have a number of early returns
that should always return a value first.

[More on exhaustiveness checking in the docs](https://typing.readthedocs.io/en/latest/guides/unreachable.html)


### Use `int | None` instead of `Optional[int]`

#### Why

No one knows why, but it's consistent.

#### Do

```python
a: int | None
```

### Don't

```python
a: Optional[int]
```


### Use the type system to encode and validate business logic constraints

Try to model business constraints in the type system, as long as it's practical.

Links for inspiration: [making illegal states unrepresentable](https://fsharpforfunandprofit.com/posts/designing-with-types-making-illegal-states-unrepresentable/),
[typestate pattern](https://cliffle.com/blog/rust-typestate/)

#### Why:

The earlier we find mistakes, the less costly they are. Using the type system lets us find errors
even before we start writing tests.

#### Do:

```python
@dataclass(frozen=True, slots=True)
class AnonymousUser:
    id: int


@dataclass(frozen=True, slots=True)
class AuthenticatedUser:
    id: int
    name: str
    email: str


class process(user: AnonymousUser | AuthenticatedUser) -> None:
    ...
```

#### Don't:

```python
@dataclass(frozen=True, slots=True)
class User:
    id: int
    name: str | None
    email: str | None


class process(user: User) -> None:
    ...
```


### Use `T_Whatever` format for meaningful type variables

Sometimes you need type variables that aren't just `T`.

#### Why:

You've got to pick one format and stick with it.

#### Do:

`T_User`

#### Don't:

* `UserT`
* `User`
* `UserType`



## Asserts

### Use `assert` for defensive coding and documentation

Whenever you assume something to be true, assert it explicitly with an `assert`.

#### Why:

`assert` serves two purposes:

* surfacing broken assumptions early and explicitly
* documenting the assumptions you're making

#### Do:

```python
def trim_nonempty_list(lst: list[T], n: int) -> list[T]:
    assert len(list) > 0
    assert 0 <= n <= len(list)

    return lst[:n]
```

#### Don't:

```python
def trim_nonempty_list(lst: list[T], n: int) -> list[T]:
    return lst[:n]
```


### Assume that `assert`s are always run

We never run Python with the `-O` flag, so `assert` can be assumed to always run.
Plan expensive checks accordingly.

#### Why:

Python assertions were initially modelled after C, where assertions are compiled out during optimised
compilation. Today this doesn't match the way Python is used in practice.

Many Python libraries use asserts to assert things that must always be true, including safety-critical conditions
(e.g. [Django](https://github.com/django/django/blob/db5980ddd1e739b7348662b07c9d91478d911877/django/contrib/auth/hashers.py#L333)).
There is little performance benefit to disabling those assertions. On the other hand, being able to assert things
to be true is good and our style should encourage this, making assertions as concise and explicit as possible.


### Use positive assertion messages

#### Why:

Positive assertion messages make the intent clearer.

#### Do:

`assert a.is_foobar(), "a is foobar"`

#### Don't:

* `assert a.is_foobar(), "Foobar error"`
* `assert a.is_foobar(), "a is not foobar"`



## Imports

### Use direct imports (with exceptions documented below)

Prefer direct imports (`from lib import f; f()`, not `import lib; lib.f()`), except when
it's too annoying or when using one of the libraries documented below.

Exceptions:

* `import pandas as pd`
* `import numpy as np`
* `import polars as pl`

#### Why:

When you use direct imports, a missing module member will throw an error during the import.
If you refer to the module instead, the error will be thrown when the module member is accessed,
potentially much later.

#### Do:

```python
from lib.datapuddle import prepare

prepare(...)
```

```python
from lib.datapuddle import prepare as datapuddle_prepare

datapuddle_prepare(...)
```

#### Don't:

```python
from lib import datapuddle

datapuddle.prepare(...)
```


### Use absolute imports

#### Why:

1. Deep relative imports (like `from ...bar import f`) are confusing
2. Some tooling doesn't work well when we mix relative and absolute imports

Therefore, we pick one style (absolute imports) and use it throughout.

This is faciliated by the root of the monorepo being in `$PYTHONPATH`.

#### Do:

```python
from data.foo.bar import f
```

#### Don't:

```python
from .bar import f
```



## Dataclasses/pydantic

### Use `dataclass` by default when values need to be grouped together

#### Why:

Slotted `dataclass`es have almost no runtime overhead, make code more readable, and
make typing more explicit.

#### Do:

```python
@dataclass(frozen=True, slots=True)
class Point:
    x: int
    y: int

POINTS: list[Point] = [Point(0, 1), Point(1, 2)]
```

```python
@dataclass(frozen=True, slots=True)
class Result:
    first: str
    second: str
    is_recognised: bool

def foo() -> Result:
    ...
    return Result(a, b, True)
```

#### Don't:

```python
POINTS: list[Point] = [(0, 1), (1, 2)]
```

```python
def foo() -> Result:
    ...
    return a, b, True
```


### Make `dataclass` frozen and slotted by default

#### Why:

`frozen` makes dataclass fields immutable, which reduces the chances of accidental mutations. Note
that nested datastructures can still be mutated, so `frozen_dataclass.a_list.append(...)` works.

`slots` [makes the class store the fields directly](https://docs.python.org/3/reference/datamodel.html#object.__slots__)
instead of going through a hashmap, which greatly reduces memory and CPU overhead of dataclasses.
Slotted dataclasses are very close to simple tuples in terms of performance.

#### Do:

```python
@dataclass(frozen=True, slots=True)
class Foo:
    ...
```

#### Don't:

```python
@dataclass
class Foo:
    ...
```


### Use `pydantic` only where validation or serialisation is required

#### Why:

Pydantic has substantially more overhead, API surface, and gotchas than raw `dataclass`es,
so we prefer `dataclass` by default and only use `pydantic` on the "edge" of the system.


## Django

### Add `created_at`/`changed_at` meta-fields to models unless there are reasons not to

#### Why:

There is little cost to an extra pair of datetime fields, and they are useful for
debugging and fixing data when sometimes goes wrong.


## Links

* [Rejected and retired guidelines](rejected_retired.md)
```


# 5. Website

The website should be available as erctpapers.com.
DNS is hosted at GoDaddy.com.

The website should be available on 80 port in production environment and as port 8080 in development. 

The following URLs should be available on the website
* /papers/: a list of all papers in HTML form (paper_list_view)
* /papers/<slug:slug>/: detail page for a specific paper in HTML
* /api/papers/: JSON list of all papers + allows POST creation
* /api/papers/<slug:slug>/: JSON detail for a single paper

The website doesn't use any client-side frameworks like React. It is done using vanila HTML + CSS + JavaScript (if needed).
The website uses bootstrap (https://getbootstrap.com/) for HTML/CSS.


# 6. ERCT Standard

```
# ERCT \- Educational RCT Standard

# Introduction to ERCT

Randomised Controlled Trials (RCTs) are often hailed as the "gold standard" in evidence-based educational research. However, the mere implementation of an RCT does not guarantee reliable or applicable results. Many educational RCTs face challenges that compromise their findings, leading to discrepancies between research outcomes and real-world applications in schools.

The Educational Randomised Controlled Trial (ERCT) Standard addresses these challenges by introducing 12 critical requirements for educational studies. This comprehensive framework, known as CETD-SAYB-GRIP, is designed to enhance the quality, rigour, and real-world applicability of educational research.

The ERCT Standard consists of three progressive levels, each comprising four criteria:

1. CETD: Class, Exam, Term, Documented  
2. SAYB: School, AllExams, Year, Balanced  
3. GRIP: Graduation, Reproduced, Independent, Pre-Reg

By adhering to these criteria, researchers can conduct more robust studies, and educators can more confidently interpret and apply research findings. This standard serves as both a guide for conducting high-quality educational RCTs and a tool for evaluating the strength of existing research.

Papers evaluated using this standard are classified based on the highest level they fully satisfy, providing a clear indicator of their methodological rigour and potential for real-world impact.

| Level 1 | Class-level RCT  | Exam-based assessment | Term duration | Documented control group |
| :---- | :---- | :---- | :---- | :---- |
| Level 2 | **S**chool-level RCT | **A**ll-subject exam-based assessment | **Y**ear duration | **B**alanced resources control group |
| Level 3 | **G**raduation follow-up  | **R**eproduced | **I**ndependently conducted | **P**re-registered protocol |

# Reporting

When evaluating a paper, provide the following:  
1\. If this paper is a Randomised Controlled Trial  
2\. Overall level achieved (1, 2, or 3\)  
3\. List of criteria met and not met at each level  
4\. For each criterion, provide either a quote showing how this criterion is met or a description of the problem explaining why you consider this criterion not met.

# 

# Level 1 ERCT

CETD (Class, Exam, Term, Documented)

## C \- Class-level RCT

\- The study must be a Randomised Controlled Trial (RCT) conducted at the class level.  
\- Randomisation should be clearly described and properly implemented.  
\- Check for: Description of randomisation process, sample size, and unit of randomisation.  
\- A stronger school-level RCT is required at Level 2\.  
\- If the study was done as a school-level RCT, then this weaker class-level criterion is considered met.

**Problem:**  
A study claims to be an RCT but assigns treatments to students within the same classroom. This can lead to contamination effects, where students in the control group are influenced by those in the treatment group, or teachers inadvertently apply intervention techniques to all students. Class-level RCT helps to ensure proper isolation of treatment and control groups, reducing interference.

**Exceptions:**  
If an intervention is designed for personal teaching like tutoring then this Class-level RCT criterion isn’t applicable and even normal student-level RCT is considered OK.

**Procedure:**

1. **Locate Randomisation Description:** Search the paper for any section describing how participants were allocated to intervention and control conditions. Extract a direct quote that explains the unit of randomisation (e.g., "Classes were randomly assigned...").  
2. **Check Unit of Randomisation:** Verify that the quote states that entire classes or school, not individual students within a single class, were randomized. If it’s unclear, look for additional quotes clarifying randomisation steps.  
3. **Exception Check (Tutoring/Personal Teaching):** If the intervention is specifically about personal tutoring or one-to-one teaching, locate a quote in the paper stating this. If such an exception is clearly described, then student-level RCT is allowed, and the criterion is satisfied.  
4. **Decision:** If the paper clearly states that randomisation was at the class level or stronger school level (or meets the exception criterion), mark this criterion as “met,” including the quotes used to verify this. If randomisation was done at the student level within a single class without a valid exception, mark as “not met” and provide the quote that shows incorrect randomisation.

## E \- Exam-based Assessment

\- The study must use standardised exam-based assessments.  
\- Assessments should not be specially designed for the study but should be standard, widely recognised tests.  
\- Check for: Names of standardised tests used, their validity and reliability, and appropriateness for the study population.  
\- What is important is that a standard exam-based assessment is used, not whether there is a positive effect.  
\- Stronger all-subject exam-based assessment is required at Level 2\.

**Problem:**  
Researchers often create a custom test specifically designed to measure the outcomes of their intervention. This can lead to bias, as the test may be overly aligned with the intervention, inflating its apparent effectiveness. Standardised exams provide a more objective and comparable measure of educational outcomes.

**Procedure:**

1. **Identify the Assessment Tool:** Locate any quotes from the paper describing the test or examination used to measure outcomes. For example: “We used the national standardised exam in mathematics…” or “We developed a new test for the purpose of this study…”  
2. **Check Standardisation:** If the exam name or description indicates it is a widely recognised standardised test (e.g., “state-wide standardised achievement test,” “national curriculum exam”), it meets the criterion. Quote the part that confirms its standardization.  
3. **If Custom Test:** If the paper states that the test was created by the researchers specifically for this study (e.g., “A bespoke test was developed…”), this fails the criterion. Provide the quote confirming the test is custom-made.  
4. **Decision:** Mark as “met” if you found a quote confirming a known standardised exam. Mark as “not met” if you found a quote confirming a custom-made assessment.

## T \- Term Duration

\- The intervention must last for at least one full academic term.  
\- A term is typically defined as a semester or equivalent (approximately 3-4 months).  
\- Check for: Clear statement of intervention duration, dates, and alignment with academic calendar.  
\- Stronger one year-long intervention duration is required at Level 2\.  
\- If the study duration was year-long then this weaker term-duration criterion is considered met.

**Problem:**  
Many studies conduct a brief, two-week intervention and immediately measures outcomes. Short-term interventions may show temporary effects that don't persist, or miss delayed effects that take time to manifest. Ensuring at least a term-long intervention allows for more reliable assessment of the intervention's impact.

**Procedure:**

1. **Find Intervention Duration:** Identify quotes from the paper specifying the start and end dates or the duration of the intervention (e.g., “The program ran from September to December…”).  
2. **Check Length:** Ensure that the quoted period covers at least one full academic term (or longer). If the paper’s academic calendar is unclear, look for quotes describing what constitutes a term in that context.  
3. **Decision:** Mark as “met” if the quoted duration is at least one full term. Mark as “not met” if the quoted duration is shorter than a term or not clearly stated.

## D \- Documented Control Group

\- The control group must be well-documented.  
\- Documentation should include demographic information, baseline performance, and any treatments received.  
\- Check for: Detailed description of control group characteristics, size, and conditions.

**Problem:**   
Many studies mention having a control group but provide no details about its composition or treatment. Why it's an issue: Without proper documentation, it's impossible to assess whether the control group was truly comparable or if it received any unintended interventions. Detailed documentation of the control group allows for proper comparison and interpretation of results.

**Procedure:**

1. **Locate Control Group Description:** Find quotes from the methods section describing the control group’s demographics, baseline performance, or any conditions placed on them. For example: “The control group received standard instruction, and included 30 students with similar demographic backgrounds…”  
2. **Assess Documentation Clarity:** Check if these quotes detail who the control group is, their baseline characteristics, and confirm that no special treatment was given beyond normal schooling. If no such descriptive quote is found, this is a failure.  
3. **Decision:** Mark as “met” if you can quote clear documentation of the control group’s characteristics. Mark as “not met” if no adequate quote describing the control group is provided.

# 

# Level 2 ERCT

SAYB (School, AllExams, Year, Balanced)

## S \- School-level RCT

\- The study must be a Randomised Controlled Trial (RCT) conducted at the school level.  
\- Randomisation should occur among schools, not just classes within schools.  
\- Check for: Description of school selection process, number of schools involved, and randomisation method.  
\- If this stronger school-level RCT criterion is met then weaker class-level RCT criterion is also considered as met.

**Problem:**   
A class-level RCT shows positive results, but when implemented school-wide, the effects disappear. Class-level randomisation might not account for school-level factors that influence the intervention's effectiveness. School-level randomisation captures a more realistic implementation scenario and accounts for school-wide factors. They are the closest to real-life implementations.

**Procedure:**

1. **Identify Randomisation Level:** Locate quotes describing the randomisation procedure at the school level (e.g., “Twenty schools were randomly assigned to either the intervention or control condition…”).  
2. **If Only Class-level or Student-level Mentioned:** If you find quotes that randomisation was at class or student level only, this criterion is not met.  
3. **Decision:** Mark as “met” if a quote confirms school-level randomisation. Mark as “not met” if no quote indicates school-level assignment.

## A \- All Exams (All-Subject Exam-Based Assessment)

\- The study must measure impact on all main subjects taught in the school, not just the subject of intervention.  
\- Only standard standardised exam-based assessments are considered (see more details in the “E \- Exam-based Assessment” criterion description).  
\- This prevents overlooking potential negative impacts on non-intervention subjects.  
\- Check for: List of all subjects assessed, description of assessment methods for each subject.  
\- If this stronger All Exams criterion is met then weaker “E \- Exam-based Assessment” criterion is also considered as met.

**Problem:**  
For example a maths intervention shows great improvement in maths scores, but researchers don't measure performance in other subjects. This intervention might be improving maths at the expense of other subjects, leading to an imbalanced education. Measuring all subjects ensures the intervention doesn't have unintended negative consequences in non-target areas.

**Exception:**  
For highly specialised interventions in upper secondary or vocational education, measuring impact on directly related subjects might be sufficient if the rationale is clearly explained.

**Procedure:**

1. **Check Subjects Assessed:** Locate quotes from the paper listing the subjects tested. For example: “We assessed student performance in math, science, and language arts at the end of the year…”  
2. **All Main Subjects Coverage:** Verify from the quotes that all main subjects taught in that educational level were assessed. If unsure what the main subjects are, refer to the paper’s curriculum description or standard subjects in that context. Make sure that they are standard standardised exam-based assessments, not some custom tests.  
3. **If Only One Subject:** If you only find quotes stating a single subject assessment (e.g., “We only measured math scores”), criterion is not met.  
4. **Exceptions:** If the paper states a clear rationale for a specialized intervention (e.g., vocational training focused solely on welding certification) and justifies measuring only related outcomes, quote that explanation and consider this acceptable.  
5. **Decision:** Mark as “met” if quoted evidence shows all main subjects (or justified exception) were assessed. Mark as “not met” if quoted evidence shows only one or a limited set of subjects without justification.

## Y \- Year Duration

\- The intervention must last for at least one full academic year.  
\- Check for: Clear statement of intervention duration covering a full academic year, with specific start and end dates.  
\- If this stronger Year Duration criterion is met then weaker “T \- Term Duration” criterion is also considered as met.

**Problem:**   
A term-long intervention shows promising results, but these gains fade by the end of the school year. Some educational interventions may have short-term effects that don't persist long-term. A year-long study is a reasonable practical compromise \- it is long-enough to have good confidence in the intervention results while still practical as schools often are organised around years.

**Procedure:**

1. **Find Duration Information:** Identify quotes specifying the intervention period. For example: “The intervention was implemented from September 2020 to June 2021.”  
2. **Check Length Against a Year:** Verify from the quotes that it covers an entire academic year (generally \~9-10 months).  
3. **Decision:** Mark as “met” if the quoted duration spans a full academic year. Mark as “not met” if quotes indicate a shorter duration.

## B \- Balanced Control Group

\- The control group must be balanced in terms of time spent on education and budget allocation.  
\- If the intervention involves increased time or budget, the control group should receive an equivalent increase to be spent on "business as usual" activities.  
\- Check for: Detailed comparison of time and resources allocated to intervention and control groups.

**Problem:**   
An intervention that provides extra tutoring time (or extra budget) shows positive results, but the control group received no additional educational time (or money). It's unclear whether the positive results are due to the specific intervention or simply the additional time or money spent on education. Ensuring the control group receives balanced time and resources isolates the effect of the specific intervention.

**Procedure:**

1. **Identify Intervention Resources:** Find quotes describing the intervention in the test group. Examples: “Students in the intervention group received an additional hour of tutoring each day.” “Teachers in the intervention group were provided with new tablets and training sessions.”  
2. **Determine if Additional Resources were Provided:** Based on the quotes, decide if these interventions required extra budget/time/resources compared to standard instruction. If uncertain, look for additional quotes clarifying the nature of the intervention. Include the detailed description of the additional resources into your explanation.   
3. **If No Additional Resources Required:** If the quotes show no extra resources (e.g., “The intervention involved a new teaching method but no additional class time or materials”), mark as “met” without further checking.  
4. **If Additional Resources Required:** Locate quotes that describe what the control group received. For example: “Control schools also received additional professional development time equivalent to the intervention group’s training hours.” Verify that the quoted resources/time for the control group matches or balances out the intervention group’s extra input.  
5. **Decision:** Mark as “met” if the quotes confirm a balanced allocation of extra time/budget/resources to the control group. Mark as “not met” if no quotes indicate any effort to balance the resources.

# 

# Level 3 ERCT

GRIP (Graduation, Reproduced, Independent, Pre-Reg)

## G \- Graduation Tracking

\- The study must follow up and track participants until their graduation.  
\- This assesses long-term impacts of the intervention.  
\- Check for: Description of follow-up methods, duration of tracking, and graduation data collection processes.

**Problem:**   
Unfortunately it often happens that an intervention shows positive short-term effects, but researchers don't follow-up to see if these benefits translate to long-term outcomes. Some interventions might have short-term benefits that don't ultimately impact important long-term educational outcomes. Tracking until graduation from the current school level provides insight into the lasting impact of interventions on students' educational journeys and is still practical as it doesn’t require tracking the students when they left this school.

**Procedure:**

1. **Find Follow-up Period:** Locate quotes describing the follow-up duration. For example: “Students were tracked through to the end of their primary education, until Grade 6 graduation.”  
2. **Check Graduation Tracking:** Confirm from the quotes that the study did not stop measurement immediately after the intervention ended, but continued until the students graduated from that educational stage.  
3. **Decision:** Mark as “met” if quoted evidence shows tracking continued through graduation. Mark as “not met” if quoted evidence shows tracking stopped earlier or no mention of graduation tracking is found.

## R \- Reproduced

\- The study must be independently replicated.  
\- Replication should ideally be conducted by a different research team in a different context.  
\- Check for: Reference to original study, description of replication process, and comparison of results.

**Problem:**   
A highly publicised educational intervention fails to show the same positive results when implemented in different schools or contexts. Single studies may have results influenced by specific contexts, leading to non-generalisable findings. There have been numerous cases in educational research where initial studies were promising, but replication efforts revealed little to no effect. Reproduction in different contexts ensures the intervention's effects are robust and generalisable.   
**Procedure:**

1. **Identify Mention of Replication:** Find quotes where the authors mention a previous or separate study that replicated their intervention and results. For example: “A subsequent study by Smith et al. (2022) implemented the same intervention in a different district and found similar effects.”  
2. **Check Independence:** Confirm from the quotes that the replication was done by a different team or institution, not the same authors.  
3. **Decision:** Mark as “met” if quoted references show independent replication in a different context. Mark as “not met” if no quotes mention replication or if the replication was by the same research team only.

## I \- Independent Conduct

\- The study must be conducted independently from the authors who designed the intervention.  
\- This reduces potential bias in implementation and analysis.  
\- Check for: Clear statement of who conducted the study and their relationship (or lack thereof) to the intervention designers.

**Problem:**   
When the researchers or authors of an intervention conduct the study themselves, there is a risk of biased reporting or analysis. For example, the authors might subconsciously or consciously influence data collection or interpretation to favour their intervention.

**Procedure:**

1. **Check Research Team Independence:** Look for quotes in the acknowledgments, methods, or author contribution sections. For example: “Data collection and analysis were conducted by an external evaluation team with no involvement in the intervention’s design.”  
2. **If Authors are the Designers:** If the quotes show that the same authors developed the intervention and also carried out the study, this criterion fails unless there is a statement of third-party oversight.  
3. **Decision:** Mark as “met” if quoted evidence confirms independence (e.g., an external evaluation agency). Mark as “not met” if quotes indicate the same team designed and tested the intervention without independent oversight.

## P \- Pre-Registered

\- The full study protocol must be pre-registered before the study begins.  
\- Pre-registration should include hypotheses, methods, and planned analyses.  
\- Check for: Link to pre-registration, date of pre-registration (must be before data collection began), and adherence to pre-registered plan.

**Problem:**   
Researchers often analyse their data in multiple ways and only report the analyses that show significant positive results. This p-hacking or selective reporting can lead to false positive results and an inflated sense of the intervention's effectiveness. Pre-registration of hypotheses and analysis plans prevents selective reporting and increases transparency in research.

**Procedure:**

1. **Locate Pre-Registration Statement:** Find quotes mentioning a registry platform (e.g., “The study was pre-registered on ClinicalTrials.gov (ID…) before data collection began.”).  
2. **Verify Timing:** Check quotes for a date of pre-registration and ensure it was before data collection started (e.g., “Pre-registration occurred in June 2020, data collection began in September 2020.”).  
3. **Decision:** Mark as “met” if quoted evidence confirms a pre-registration reference and timing. Mark as “not met” if no quotes referencing pre-registration are found or if the quoted timing indicates registration occurred after data collection.

```


# 7. Examples of paper JSON

```
{
  "title": "Improving quality of teaching and learning in classes by using augmented reality video",
  "authors": "Joanne Yip, Sze-Ham Wong, Kit-Lun Yick, Kannass Chan, Ka-Hing Wong",
  "url": "https://doi.org/10.1016/j.compedu.2018.09.014",
  "slug": "yip-et-al-2018-ar-video",
  "abstract": "This study compares a traditional handout versus an augmented reality (AR) video approach to teaching threading and knitting concepts in textile-related courses. The authors randomly split students into two groups (handout vs. AR video) and measured learning outcomes via short tests, threading performance, time-on-task, and questionnaire responses. Results suggest that AR-based instruction increases students’ understanding of complex 3D processes and reduces the time required to learn certain tasks. The paper describes the randomized assignment, the educational context, test details, and data analysis of pre- and post-tests, concluding that AR video use is a promising instructional tool for textile and clothing students.",
  "publication_date": "2018-09-23",
  "erct_level": 0,
  "rct": true,
  "criteria": {
    "c": {
      "analysis": "The paper explicitly states that a randomized controlled trial (RCT) was run, with students placed into two groups and different tutorial sessions. One group received a handout (control) and the other used AR videos (intervention). Quotes from the paper include: “To reduce selection bias and obtain more accurate results, a randomized controlled trial (RCT) was run. Study participants were randomly allocated to two different groups...” (Section 2.2) and “All participants were blind to the study in that they did not have information about the learning tool until they started their tutorial class.” (Section 2.3.1). The paper also explains that “...all students were randomly allocated into different Intimate Textiles and Accessories classes by the PolyU computer system.” (Section 2.3.2).\n\nThese statements demonstrate that randomization was performed at the tutorial-class level rather than assigning individual students within the same classroom to different conditions. In other words, each tutorial class as a whole was allocated to either the AR-video condition or the handout condition. This cluster-level approach (by class) meets the requirement for Criterion C, which is that entire classes (or a stronger school-level approach) are assigned to treatment or control.\n\nNo evidence in the paper suggests that they randomized individual students within a single class, and there is no indication that the intervention is solely for individualized tutoring or personal teaching. Hence, the study meets the Class-level RCT criterion because they used whole-class randomization (albeit across different tutorial sessions). Therefore, Criterion C is satisfied.",
      "met": true,
      "explanation": "They clearly randomized at the class (tutorial session) level, satisfying the requirement for Class-level RCT.",
      "quote": "“...a randomized controlled trial (RCT) was run. Study participants were randomly allocated... all students were randomly allocated into different classes by the PolyU computer system.”"
    },
    "e": {
      "analysis": "This criterion requires that the outcomes be measured using a well-known, standardized exam. However, the paper describes using custom-made tests: “The test comprised two sections… a score of 16 was given if the needle movement steps were arranged correctly…” (Section 2.3.2) and “To evaluate the learning efficiency… a pre-test was carried out before the students received the handout or downloaded the AR videos… The time required for each student to finish the task was recorded…” (Section 2.4.2.1). These tests appear to be tailored specifically to measure students’ ability to thread or operate textile machinery and understand needle movements.\n\nThere is no mention of an officially recognized standardized assessment such as a state-wide exam, a national standardized test, or an externally validated measure with known psychometric properties. Instead, the researchers constructed their own short quizzes and performance tasks to evaluate threading and knitting knowledge. While these custom tools may be valid for local instructional purposes, they do not fulfill the criterion of a standard exam-based assessment recognized beyond the immediate study context.\n\nBecause the authors rely on researcher-designed tests and practical skill measures (time to complete threading, correct steps, etc.) rather than a widely recognized standardized exam, the requirement of using exam-based assessments (Criterion E) is not met.",
      "met": false,
      "explanation": "They used custom-created quizzes and performance metrics instead of any external standardized exam.",
      "quote": "“A simple test on three main types of knitting needles… The test comprised two sections… a score of 16 was given… The students were required to thread a flat sewing machine… the time required for each student to finish the task was recorded.” (Sections 2.3–2.4)"
    },
    "t": {
      "analysis": "To satisfy Criterion T, the intervention should span at least an entire academic term (approximately 3–4 months). In this paper, the authors describe a relatively brief set of learning trials: “A simple test… was given on 15 September 2017… Another trial on basic terminology and understanding of the threading process was designed… from 26 to 29 September 2017.” (Sections 2.3, 2.4). This short timescale does not amount to a full semester or term.\n\nThe paper makes no mention of a sustained implementation over multiple months. Instead, the reported timeline suggests that the AR intervention and handout-based intervention were each introduced and tested in a short timeframe, covering only a single session or a few workshop days. This compressed schedule is typical for a small-scale instructional study but does not align with the requirement that the intervention run for one academic term.\n\nHence, because the intervention and data collection appear to have occurred within days or weeks rather than an entire term, this criterion is not met.",
      "met": false,
      "explanation": "The study took place over brief sessions in September rather than covering a full academic term of at least several months.",
      "quote": "“A simple test on… was given on 15 September 2017… The threading task in sewing workshops… from 26 to 29 September 2017.” (Sections 2.3–2.4)"
    },
    "d": {
      "analysis": "Criterion D requires a clearly documented control group with baseline characteristics and conditions. This paper contrasts Group A (handout) with Group B (AR videos). They provide the size of each group: “Forty-six freshmen registered… 27 in Group A and 19 in Group B” (Section 2.4.1). They also give some demographic details: “All participants were… 4 males and 42 females” (Section 2.4.1), plus baseline knowledge or skill was assessed via a pre-test. For example, “Regardless of their allocated group, all students had to finish the task [threading] but were not given a time limit… Their score… was recorded… The aim was to assess the learning outcomes of both groups…” (Sections 2.3.2, 2.4.2).\n\nThey similarly document that the control group used only a written handout, describing their experiences, while the AR group had the app-based approach. They also show the pre-test and post-test comparisons, clarifying that the control group spent about the same amount of time in the workshop but had no AR videos. The authors highlight the control group’s results for each measure (time to thread, final test scores, etc.) and mention that the control group needed more time and found the process more difficult.\n\nOverall, the authors do identify the control group’s composition, pre-test performance, and the conditions they received (no AR technology). This level of detail satisfies Criterion D’s requirement for a documented control group, even though it is not as extensive as large-scale demographic tables might be. They do report group sizes, pre-test scores, and confirm that the control group simply used a standard handout, facilitating clarity on what the control group actually experienced.",
      "met": true,
      "explanation": "They provide basic demographic info, pre- and post-test scores, and clearly note that the control group used only a handout with no AR resources.",
      "quote": "“...46 freshmen… 27 were placed in Group A (handouts), 19 in Group B (AR videos). All participants were… 4 males and 42 females… a pre-test was carried out before the students received the handout or downloaded the AR videos…” (Sections 2.4.1–2.4.2)"
    },
    "s": {
      "analysis": "This Level 2 criterion (S) requires randomization at the school level, meaning entire schools (not just classes) are assigned to intervention or control. The paper focuses on students at one institution (The Hong Kong Polytechnic University) and describes random assignment of entire tutorial classes or sessions, but not entire schools. Quotes reference “...all students were randomly allocated into different Intimate Textiles and Accessories classes by the PolyU computer system...” but do not mention multiple schools.\n\nSince the study took place in a single university department using tutorial-class groupings rather than an inter-school design, it does not meet the threshold for school-level RCT. The standard for S is typically relevant to K–12 or multi-school educational contexts, which is not the case here.\n\nHence, while they do have a cluster randomization approach at the level of tutorial classes, they are not randomizing entire schools. This means Criterion S is not satisfied.",
      "met": false,
      "explanation": "They randomized classes within one university department, not entire schools.",
      "quote": "No quotes indicate multi-school assignment; they mention only that students were randomly allocated into tutorial classes."
    },
    "a": {
      "analysis": "Criterion A (AllExams) requires measuring outcomes in all main academic subjects (or a justified subset if it is vocational) using standard exams, to check for unintended negative effects in other domains. The paper focuses exclusively on tasks related to knitting needles, threading, and sewing machine operations within textiles/clothing courses. There is no indication that the authors assessed learning in other subject areas (such as mathematics, writing skills, or other core disciplines). Instead, the study measures only the outcome of improved performance in operating textile machinery.\n\nWhile the approach is understandable for a specialized course in textiles, it does not meet the requirement for a comprehensive, all-subject exam-based assessment. The authors never purport to measure broader learning or other academic subjects. Therefore, Criterion A is not met.\n\nIt is worth noting that the authors primarily aimed to see if AR-based instruction improved a specific skill (threading and needle comprehension). This narrower scope is valid for their goals but does not fulfill “AllExams.”",
      "met": false,
      "explanation": "They evaluated only sewing/knitting performance and knowledge, with no mention of measuring every main subject area via standardized exams.",
      "quote": "“...the study focused on the evaluation of an AR application in education… the innovation… particularly for learning a complicated task/concept, i.e., threading.” (Section 5)"
    },
    "y": {
      "analysis": "Criterion Y requires that the intervention last for at least one full academic year. The paper describes a short study spanning only a few weeks in September. Specifically, “...the simple test on knitting needles was given on 15 September...” followed by a threading task at the end of the same month. There is no mention of continuing the intervention for an entire year.\n\nBecause the study only covered a brief period (likely days or weeks), it does not fulfill the year-long requirement. The authors mention no extended follow-up or multiple months of AR usage. Therefore, Y is not met.\n\nShort classroom experiments are common in educational research but do not meet the stricter demands of a year-long intervention.",
      "met": false,
      "explanation": "The entire intervention and data collection took place over a short timeframe, not spanning a full academic year.",
      "quote": "“A simple test… 15 September… Another trial… 26 to 29 September… The time required for each student… was recorded.” (Sections 2.3–2.4)"
    },
    "b": {
      "analysis": "To meet Criterion B, if the treatment group receives extra resources (budget, technology, or instructional time), then the control group should receive an equivalent resource injection to rule out mere resource differences. The authors describe that the intervention group used a specially developed AR mobile application, while the control group only had a handout. The AR group presumably had more technological resources (an app, phone usage, special content). There is no mention of providing the control group with any comparable resource or time balance.\n\nThe paper does not describe an attempt to equate total budgets or instructional time given to each group, e.g., the control group receiving some parallel digital resource or extra budget. The authors simply provided Group B with an AR app and Group A with a typical printed handout. That mismatch is precisely what B warns about: it can be unclear if the AR effect is due to novelty or additional resources. While this design can be valid for research, it does not satisfy the Balanced Control Group requirement.\n\nHence, B is not met because there is no equivalently augmented resource for the control group to isolate the specific effect of AR from extra technology/time/funding factors.",
      "met": false,
      "explanation": "The intervention group had AR technology, but the control group only had a handout and no equivalently increased resources or budget.",
      "quote": "“Group A (handout)… Group B (AR video)… no mention of matching resources or budget for the control group.” (Sections 2.4.2–2.4.3)"
    },
    "g": {
      "analysis": "Criterion G (Graduation Tracking) calls for following students until they graduate from that school level, ensuring a long-term outcome measure. This paper reports only immediate effects on test performance and skill demonstration after a short learning intervention. For example, “When the students finished threading, their time spent… and their score were recorded.” (Section 2.4.2.2).\n\nThe authors do not describe any follow-up or tracking beyond that immediate post-intervention test. They do not mention seeing if these students eventually graduated or if the gains were retained. Therefore, the requirement to track participants through graduation is not met.\n\nBecause the data collection ended soon after the short intervention, G is not satisfied.",
      "met": false,
      "explanation": "They gathered only short-term data from immediate post-tests, with no extended tracking through graduation.",
      "quote": "“Another trial on basic terminology and understanding… The results obtained in the simple test… were then assessed… [no mention of following them to graduation].” (Sections 2.3–2.4)"
    },
    "r": {
      "analysis": "Criterion R requires that the study be reproduced by an independent team in a different context, demonstrating replication. The paper does not mention any independent replication attempts or other published studies implementing the same AR approach with a new population by a separate research group. They only present their own single study design at The Hong Kong Polytechnic University.\n\nNo references describe follow-up replications or expansions by others. Thus, R is not met.\n\nWhile the authors mention future directions, the text does not indicate that any external group has repeated the same method independently. Therefore, the requirement for an independently reproduced study is not satisfied.",
      "met": false,
      "explanation": "No independent replication is reported or cited; they only describe their single study in one institution.",
      "quote": "The paper does not mention any separate or replicated study by another research team."
    },
    "i": {
      "analysis": "Criterion I (Independent Conduct) requires that the study be conducted by a team independent from the intervention’s designers. Here, the authors themselves created the AR app called “ITC VR AR” as part of their program: “To apply AR to the learning environment, a mobile application (app) called ITC VR AR was developed by the Institute of Textiles and Clothing (ITC)…” (Section 2.2). The same authors—particularly from the Institute of Textiles and Clothing—then tested the efficacy of this newly developed AR tool.\n\nThe text does not explicitly mention an external or neutral evaluator collecting data or performing the analysis. Instead, it appears that the same institution developed the AR app and measured its effectiveness, which raises the possibility of partial or unintentional bias in favor of their own intervention. There is no statement that a separate, independent organization ran the evaluation.\n\nBecause the authors were directly responsible for both the intervention design (the AR app) and the entire research process, the study does not satisfy the requirement that it be independently conducted by a team unaffiliated with the intervention. Hence, I is not met.",
      "met": false,
      "explanation": "The same group that designed the AR app also conducted the research, with no independent evaluation team.",
      "quote": "“A mobile application (app) called ITC VR AR was developed by the Institute of Textiles and Clothing… The aim… to determine whether AR videos can improve teaching.” (Sections 2.2, 2.5)"
    },
    "p": {
      "analysis": "Criterion P (Pre-Registered) requires that the study protocol, hypotheses, and analysis plan be published or posted in a recognized registry prior to data collection. The paper does not reference any pre-registration (e.g., a date, a registry ID, or a mention of prospective registration). Instead, it simply describes the design, method, and results as an original study.\n\nNo statements indicate that the authors posted a formal plan ahead of time, nor do they cite OSF, ClinicalTrials.gov, or any similar platform. Thus, there is no evidence of pre-registration, and P is not met.\n\nWhile this does not invalidate the study’s findings, it means they did not complete the pre-registration process that ensures transparency around planned analyses.",
      "met": false,
      "explanation": "They did not mention registering their protocol before data collection; no public registry link or ID is cited.",
      "quote": null
    }
  }
}
```

# 8. AWS Infrastructure

AWS root user: 
login: 9353867@gmail.com
Account ID: 4900-0464-3092
Organization: erctpapers

Deployment user: 
Login: deployment-user
Console sign-in URL: https://490004643092.signin.aws.amazon.com/console

Django super user:
login: root

RDS Database:
DB identifier: erct-papers-db
Region & AZ: eu-north-1c
Endpoint: erct-papers-db.c964ku2seiin.eu-north-1.rds.amazonaws.com
Port: 5432
Master username: django_user

# 9. About Myself

I develop software for 20 years, I managed various projects as a team leader.
I started from Java, C, C++, JavaScript. I have good experience with HTML and CSS but my knowledge 
is outdated here, I stopped actively developing frontend in 2010. I have good experience with Python 
but not with Django (I am new to Django). One of the goals of this project for me is to learn Django and Django Ninja API.
I have some experience with AWS (about one year) but I am not an expert.
Please explain me Django and AWS while you output code, no need to explain it separately, ony by commenting what you do in this project. 
No need to explain Python, I am good with Python.

My name is Vassili Philippov.
My email: vassili@glite.ai. 

# 10. About Python code

## 10.1 Paper model

I want the Paper model to include each of the 12 criteria from CETD-SAYB-GRIP as a group of three fields like:
```
# Criterion C (Class-level randomization)
c_met = models.BooleanField(default=False)
c_explanation = models.TextField(blank=True)
c_quote = models.TextField(blank=True)
```

# 11. Using UV package manager

The project uses uv package manager.

The uv package manager is a fast, Rust-powered tool designed to replace pip, pip-tools, pipx, and even Poetry by unifying dependency resolution, environment management, and tool installation. Unlike Poetry, uv is built to work with a pure PEP 621 configuration—where project metadata and dependency declarations reside in the standard [project] table of your pyproject.toml file. With uv, you define your runtime dependencies under [project].dependencies and any optional or development dependencies under [project.optional-dependencies] or in dependency groups (as defined by PEP 735). In addition, uv supports alternative dependency sources (Git, URL, path, workspace) in a dedicated [tool.uv.sources] table.

Installation of uv is straightforward. You can install it globally via pip with the command pip install uv, or use the standalone installer available via a curl script from astral.sh/uv/install.sh (for macOS and Linux) or the corresponding PowerShell script for Windows. Once installed, uv provides a rich set of commands: for example, uv init creates a new project, uv add lets you add dependencies (with options like --dev, --group, or --optional), and uv sync installs your project dependencies into your virtual environment. uv also creates a lockfile (typically uv.lock) that ensures reproducibility of your dependency tree.

For Docker integration, your Dockerfile should copy the pyproject.toml (and optionally uv.lock) files before the rest of your source code to leverage Docker’s caching layers. Then, install uv via pip and run uv sync --no-dev (or include dev dependencies if needed) to install your project’s dependencies as declared in the PEP 621–compliant pyproject.toml. After the dependencies are installed, the rest of your source code is copied into the container, and the container is configured to run Gunicorn to serve your Django application on the required production port (typically port 80).

Your pyproject.toml should follow the PEP 621 standard by including the [project] section with keys like name, version, description, authors, requires-python, readme, license, and dependencies. Optional dependencies (for development or extras) go into [project.optional-dependencies]. Optionally, you can include a [tool.uv] section (for example, to set default dependency groups via default-groups = ["dev"]), although if you wish to avoid vendor-specific sections you can omit it and instead use uv’s command-line flags (like --extra dev) when running uv sync. This approach keeps your configuration vendor-neutral while still allowing you to benefit from uv’s rapid dependency resolution, alternative sources, and environment management features.

In summary, by using uv you obtain a unified, fast, and reproducible dependency management workflow for your Django project. The tool supports standard PEP 621 metadata in your pyproject.toml, alternative dependency sources, and dependency groups—all of which simplify Docker builds and local development. With uv installed (via pip or standalone installer), your Dockerfile can simply run uv sync to populate the environment before starting Gunicorn, ensuring that your production container matches the dependencies declared in your PEP 621–compliant configuration.


# 12. Existing files

Please use the following files, unless there are significant reasons to change them.
If you decide to change them, please explain and comment why and what you changed and minimize the changes.

## 12.1 Dockerfile
```
# Use the official Python 3.12 slim image as the base.
FROM python:3.12-slim

# Prevent Python from writing.pyc files and enable unbuffered logging.
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

# Install system dependencies 
RUN apt-get update && apt-get install -y netcat-openbsd && rm -rf /var/lib/apt/lists/*

# Set the working directory.
WORKDIR /app
ENV PYTHONPATH=/app

# Copy the entire project directory first
COPY . /app/ 

# Install uv globally.
RUN pip install --upgrade pip && pip install uv

# Use uv to install project dependencies
# RUN uv sync --no-dev
RUN pip install -e .

# Expose port 80 (production port).
EXPOSE 80

# Run Gunicorn to serve your Django application.
CMD ["gunicorn", "mysite.wsgi:application", "--bind", "0.0.0.0:80"]
```

## 12.2 pyproject.toml
```
[project]
name = "erct-papers-website"
version = "0.1.0"
description = "A Django-based website and API for hosting and evaluating ERCT papers according to the ERCT standard."
authors = [
  { name="Vassili Philippov", email="vassili@glite.ai" },
]

requires-python = ">=3.12"
readme = "README.md"

dependencies = [
  "Django>=4.2,<5.0",
  "psycopg2-binary>=2.9.0,<3.0",
  "django-ninja>=1.0.1",
  "gunicorn>=20.1.0",
]

[project.optional-dependencies]
dev = [
  "docker",
  "django-stubs[compatible-mypy]",
  "mypy",
  "ruff",
]

[tool.setuptools]
packages = ["app"]

[build-system]
requires = ["setuptools>=58.0.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.ruff]
line-length = 100
exclude = ["**/migrations/", ".ipynb"]

[project.urls]
Homepage = "https://github.com/vassiliphilippov/erctpaperswebsite"
```

## 12.3 .gitignore
```
# OS and IDE files
.DS_Store
Thumbs.db
**/.vscode/
.idea/
.direnv/
.python-version

# Python build artifacts and caches
__pycache__/
*.py[cod]
*$py.class

# Virtual environments (common names)
.venv/
venv/

# Environment files and secrets (ignore any file starting with .env)
.env*
.secrets
env-secrets.env

# Django-specific files (if you ever use SQLite or generate static/media files locally)
db.sqlite3
static/
media/

# Docker directories (if you create these for container configurations or volumes)
.docker/
docker_volumes/
docker-volumes/

# Logs and temporary files
logs/
*.log

# Coverage, testing, and type checking artifacts
.coverage
.tox/
.nox/
*.cover
*.py,cover
.dmypy.json
.pyre/

# Miscellaneous caches and dependency resolutions
Package.resolved
.uv_cache/
```

## 12.4 docker-compose.yml
```
services:
  web:
    build: .
    container_name: erct_django_web
    volumes:
      - .:/app
    ports:
      - "8080:8080"
    env_file:
      - .env.dev
    depends_on:
      - db
    command: >
      bash -c "
        echo 'Waiting for PostgreSQL...' &&
        while ! nc -z db 5432; do sleep 0.1; done &&
        python app/manage.py migrate &&
        python app/manage.py runserver 0.0.0.0:8080
      "

  db:
    image: postgres:15
    container_name: erct_postgres_db
    env_file:
      - .env.dev
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER}"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s

volumes:
  pgdata:
```

## 12.5 docker-compose.prod.yml
```
services:
  web:
    build: .
    container_name: erct_django_web_prod
    ports:
      - "80:80"
    env_file:
      - .env.prod
    # No dependency on a local database because we rely on external RDS.
    command: >
      bash -c "
        python app/manage.py migrate --noinput &&
        gunicorn --chdir app mysite.wsgi:application --bind 0.0.0.0:80
      "
```

## 12.6 .env.dev
```
# Django settings
DEBUG=1
SECRET_KEY=dev-secret-key
ALLOWED_HOSTS=localhost,127.0.0.1

# PostgreSQL settings for development
POSTGRES_DB=erctpapers_dev
POSTGRES_USER=django_user
POSTGRES_PASSWORD=devpassword
POSTGRES_HOST=db
POSTGRES_PORT=5432
```

## 12.7 .env.prod
```
# Django settings
DEBUG=True
SECRET_KEY=<masked>
ALLOWED_HOSTS=erctpapers.com

# PostgreSQL settings for development
POSTGRES_DB=erctpapers_prod
POSTGRES_USER=django_user
POSTGRES_PASSWORD=<masked>
POSTGRES_HOST=erct-papers-db.c964ku2seiin.eu-north-1.rds.amazonaws.com
POSTGRES_PORT=5432
```

## 12.8 app/mysite/settings.py
```
import os
from pathlib import Path

# Build paths inside the project: BASE_DIR points to the project root.
BASE_DIR = Path(__file__).resolve().parent.parent

# SECURITY WARNING: keep the secret key used in production secret!
# For development, we use a fallback key if none is provided.
SECRET_KEY = os.environ.get("SECRET_KEY", "insecure-secret-key-for-dev")

# DEBUG mode is determined by an environment variable.
# Here we convert the value to an integer and then to a boolean.
DEBUG = bool(int(os.getenv("DEBUG", "1")))  # Default to True for development

# Allowed hosts are set via an environment variable.
# Now we split on commas since our .env files use comma-separated values.
ALLOWED_HOSTS = os.environ.get("ALLOWED_HOSTS", "localhost,127.0.0.1").split(",")

# Application definition.
INSTALLED_APPS = [
    "django.contrib.admin",
    "django.contrib.auth",
    "django.contrib.contenttypes",
    "django.contrib.sessions",
    "django.contrib.messages",
    "django.contrib.staticfiles",
    "papers",          # Our custom application.
    "django_ninja",    # Added for API endpoints.
]

MIDDLEWARE = [
    "django.middleware.security.SecurityMiddleware",
    "django.contrib.sessions.middleware.SessionMiddleware",
    "django.middleware.common.CommonMiddleware",
    "django.middleware.csrf.CsrfViewMiddleware",
    "django.contrib.auth.middleware.AuthenticationMiddleware",
    "django.contrib.messages.middleware.MessageMiddleware",
    "django.middleware.clickjacking.XFrameOptionsMiddleware",
]

ROOT_URLCONF = "mysite.urls"

# Templates configuration.
# We use a custom templates directory located inside the 'papers' app.
TEMPLATES = [
    {
        "BACKEND": "django.template.backends.django.DjangoTemplates",
        "DIRS": [BASE_DIR / "papers" / "templates"],
        "APP_DIRS": True,
        "OPTIONS": {
            "context_processors": [
                "django.template.context_processors.debug",
                "django.template.context_processors.request",
                "django.contrib.auth.context_processors.auth",
                "django.contrib.messages.context_processors.messages",
            ],
        },
    },
]

WSGI_APPLICATION = "mysite.wsgi.application"

# Database configuration using environment variables.
DATABASES = {
    "default": {
        "ENGINE": "django.db.backends.postgresql",
        "NAME": os.environ.get("POSTGRES_DB", "erctpapers"),
        "USER": os.environ.get("POSTGRES_USER", "django_user"),
        "PASSWORD": os.environ.get("POSTGRES_PASSWORD", "yourpassword"),
        "HOST": os.environ.get("POSTGRES_HOST", "db"),  # Commonly used in Docker setups.
        "PORT": os.environ.get("POSTGRES_PORT", "5432"),
    }
}

# Password validation settings.
AUTH_PASSWORD_VALIDATORS = [
    {"NAME": "django.contrib.auth.password_validation.UserAttributeSimilarityValidator",},
    {"NAME": "django.contrib.auth.password_validation.MinimumLengthValidator",},
    {"NAME": "django.contrib.auth.password_validation.CommonPasswordValidator",},
    {"NAME": "django.contrib.auth.password_validation.NumericPasswordValidator",},
]

# Internationalization settings.
LANGUAGE_CODE = "en-us"
TIME_ZONE = "UTC"
USE_I18N = True
USE_TZ = True

# Static files configuration.
STATIC_URL = "/static/"
STATIC_ROOT = BASE_DIR / "static"

# Default primary key field type.
DEFAULT_AUTO_FIELD = "django.db.models.BigAutoField"
```

## 12.9 app/mysite/urls.py
```
from django.contrib import admin
from django.urls import path, include
from papers.api import api

urlpatterns = [
    path("admin/", admin.site.urls),
    path("api/", api.urls),  # Django Ninja API endpoints
    path("papers/", include("papers.urls")),
]
```

## 12.10 app/papers/views.py
```
from django.shortcuts import render, get_object_or_404
from django.http import HttpRequest, HttpResponse
from papers.models import Paper


def paper_list_view(request: HttpRequest) -> HttpResponse:
    # Retrieve all papers ordered by newest first (assumes a 'created_at' field exists).
    papers = Paper.objects.all().order_by("-created_at")
    return render(request, "papers/paper_list.html", {"papers": papers})


def paper_detail_view(request: HttpRequest, slug: str) -> HttpResponse:
    # Retrieve a specific paper based on its slug.
    paper = get_object_or_404(Paper, slug=slug)
    return render(request, "papers/paper_detail.html", {"paper": paper})
```

## 12.11 app/papers/models.py
```
from django.db import models
from django.urls import reverse
from django.utils.text import slugify

class Paper(models.Model):
    # Basic metadata fields
    title = models.CharField(max_length=255)
    authors = models.TextField()  # Using TextField allows for a longer list of authors.
    url = models.URLField(blank=True)
    slug = models.SlugField(max_length=300, unique=True, blank=True)
    abstract = models.TextField(blank=True)
    publication_date = models.DateField(null=True, blank=True)
    rct = models.BooleanField(default=False)
    erct_level = models.PositiveSmallIntegerField(
        default=0,
        help_text="ERCT level (0=Not evaluated, 1=Level1, 2=Level2, 3=Level3)"
    )

    ##########################################################
    # 12 Criteria from CETD-SAYB-GRIP:
    # For each criterion: a boolean (met or not),
    # plus two text fields for explanation and quote.
    ##########################################################

    # Criterion C (Class-level randomization)
    c_met = models.BooleanField(default=False)
    c_explanation = models.TextField(blank=True)
    c_quote = models.TextField(blank=True)

    # Criterion E (Exam-based assessment)
    e_met = models.BooleanField(default=False)
    e_explanation = models.TextField(blank=True)
    e_quote = models.TextField(blank=True)

    # Criterion T (Term duration)
    t_met = models.BooleanField(default=False)
    t_explanation = models.TextField(blank=True)
    t_quote = models.TextField(blank=True)

    # Criterion D (Documented control group)
    d_met = models.BooleanField(default=False)
    d_explanation = models.TextField(blank=True)
    d_quote = models.TextField(blank=True)

    # Criterion S (School-level randomization)
    s_met = models.BooleanField(default=False)
    s_explanation = models.TextField(blank=True)
    s_quote = models.TextField(blank=True)

    # Criterion A (All-exams)
    a_met = models.BooleanField(default=False)
    a_explanation = models.TextField(blank=True)
    a_quote = models.TextField(blank=True)

    # Criterion Y (Year-long duration)
    y_met = models.BooleanField(default=False)
    y_explanation = models.TextField(blank=True)
    y_quote = models.TextField(blank=True)

    # Criterion B (Balanced control group)
    b_met = models.BooleanField(default=False)
    b_explanation = models.TextField(blank=True)
    b_quote = models.TextField(blank=True)

    # Criterion G (Graduation tracking)
    g_met = models.BooleanField(default=False)
    g_explanation = models.TextField(blank=True)
    g_quote = models.TextField(blank=True)

    # Criterion R (Reproduced)
    r_met = models.BooleanField(default=False)
    r_explanation = models.TextField(blank=True)
    r_quote = models.TextField(blank=True)

    # Criterion I (Independent conduct)
    i_met = models.BooleanField(default=False)
    i_explanation = models.TextField(blank=True)
    i_quote = models.TextField(blank=True)

    # Criterion P (Pre-registered protocol)
    p_met = models.BooleanField(default=False)
    p_explanation = models.TextField(blank=True)
    p_quote = models.TextField(blank=True)

    # Meta fields for record keeping
    created_at = models.DateTimeField(auto_now_add=True)
    changed_at = models.DateTimeField(auto_now=True)

    def __str__(self) -> str:
        return f"Paper: {self.title} (ERCT level {self.erct_level})"

    def get_absolute_url(self) -> str:
        return reverse("paper_detail", kwargs={"slug": self.slug})

    def save(self, *args, **kwargs):
        # Ensure the ERCT level is non-negative.
        assert self.erct_level >= 0, "ERCT Level must be non-negative"
        # Auto-generate slug from the title if not provided.
        if not self.slug:
            self.slug = slugify(self.title)
        super().save(*args, **kwargs)
```

## 12.12 app/papers/schemas.py
```
from datetime import date
from ninja import Schema


class CriterionSchema(Schema):
    met: bool
    explanation: str
    quote: str


class CriteriaSchema(Schema):
    c: CriterionSchema
    e: CriterionSchema
    t: CriterionSchema
    d: CriterionSchema
    s: CriterionSchema
    a: CriterionSchema
    y: CriterionSchema
    b: CriterionSchema
    g: CriterionSchema
    r: CriterionSchema
    i: CriterionSchema
    p: CriterionSchema


class PaperSchema(Schema):
    title: str
    authors: str
    url: str
    slug: str
    abstract: str
    publication_date: date | None = None
    erct_level: int
    rct: bool
    criteria: CriteriaSchema


class PaperCreateSchema(Schema):
    title: str
    authors: str
    url: str = ""
    abstract: str = ""
    publication_date: date | None = None
    erct_level: int = 0
    rct: bool = False
    criteria: CriteriaSchema | None = None 
```

## 12.13 app/papers/api.py
```
from django.shortcuts import get_object_or_404
from ninja import NinjaAPI

from papers.models import Paper
from papers.schemas import PaperSchema, PaperCreateSchema


api = NinjaAPI(
    title="ERCT Papers API",
    version="1.0.0",
    description="API for accessing and creating ERCT paper evaluations",
    urls_namespace="papers_api"
)


@api.get("/papers", response=list[PaperSchema], summary="Get all papers")
def list_papers(request):
    papers = Paper.objects.all().order_by("-created_at")
    return papers


@api.get("/papers/{slug}", response=PaperSchema, summary="Get a paper by slug")
def get_paper(request, slug: str):
    paper = get_object_or_404(Paper, slug=slug)
    return paper


@api.post("/papers", response=PaperSchema, summary="Create a new paper")
def create_paper(request, payload: PaperCreateSchema):
    paper = Paper(
        title=payload.title,
        authors=payload.authors,
        url=payload.url,
        abstract=payload.abstract,
        publication_date=payload.publication_date,
        erct_level=payload.erct_level,
        rct=payload.rct,
    )

    if payload.criteria:
        for key in ["c", "e", "t", "d", "s", "a", "y", "b", "g", "r", "i", "p"]:
            criterion = getattr(payload.criteria, key)
            setattr(paper, f"{key}_met", criterion.met)
            setattr(paper, f"{key}_explanation", criterion.explanation)
            setattr(paper, f"{key}_quote", criterion.quote)

    paper.save()
    return paper 
```

## 12.14 app/papers/serializers.py
```
from typing import Any
from papers.models import Paper

def paper_to_json(paper: Paper) -> dict[str, Any]:
    """
    Convert a Paper instance to the JSON format requested.
    """
    return {
        "title": paper.title,
        "authors": paper.authors,
        "url": paper.url,
        "slug": paper.slug,
        "abstract": paper.abstract,
        # Convert date to ISO format or None if publication_date is empty
        "publication_date": paper.publication_date.isoformat() if paper.publication_date else None,
        "erct_level": paper.erct_level,
        "rct": paper.rct,
        "criteria": {
            "c": {
                "met": paper.c_met,
                "explanation": paper.c_explanation,
                "quote": paper.c_quote,
            },
            "e": {
                "met": paper.e_met,
                "explanation": paper.e_explanation,
                "quote": paper.e_quote,
            },
            "t": {
                "met": paper.t_met,
                "explanation": paper.t_explanation,
                "quote": paper.t_quote,
            },
            "d": {
                "met": paper.d_met,
                "explanation": paper.d_explanation,
                "quote": paper.d_quote,
            },
            "s": {
                "met": paper.s_met,
                "explanation": paper.s_explanation,
                "quote": paper.s_quote,
            },
            "a": {
                "met": paper.a_met,
                "explanation": paper.a_explanation,
                "quote": paper.a_quote,
            },
            "y": {
                "met": paper.y_met,
                "explanation": paper.y_explanation,
                "quote": paper.y_quote,
            },
            "b": {
                "met": paper.b_met,
                "explanation": paper.b_explanation,
                "quote": paper.b_quote,
            },
            "g": {
                "met": paper.g_met,
                "explanation": paper.g_explanation,
                "quote": paper.g_quote,
            },
            "r": {
                "met": paper.r_met,
                "explanation": paper.r_explanation,
                "quote": paper.r_quote,
            },
            "i": {
                "met": paper.i_met,
                "explanation": paper.i_explanation,
                "quote": paper.i_quote,
            },
            "p": {
                "met": paper.p_met,
                "explanation": paper.p_explanation,
                "quote": paper.p_quote,
            },
        }
    }

def json_to_paper(body: dict[str, Any]) -> Paper:
    paper = Paper(
        title=body.get("title", ""),
        authors=body.get("authors", ""),
        url=body.get("url", ""),
        abstract=body.get("abstract", ""),
    )

    # Parse and set date if provided
    publication_date_str = body.get("publication_date")
    if publication_date_str:
        import datetime
        try:
            paper.publication_date = datetime.date.fromisoformat(publication_date_str)
        except ValueError:
            pass

    paper.erct_level = body.get("erct_level", 0)
    paper.rct = body.get("rct", 0)

    # Criteria
    criteria = body.get("criteria", {})
    for key in ["c","e","t","d","s","a","y","b","g","r","i","p"]:
        crit_data = criteria.get(key, {})
        met = crit_data.get("met", False)
        explanation = crit_data.get("explanation", "")
        quote = crit_data.get("quote", "")
        setattr(paper, f"{key}_met", met)
        setattr(paper, f"{key}_explanation", explanation)
        setattr(paper, f"{key}_quote", quote)    

    return paper
```

## 12.15 .dockerignore
```
.git
__pycache__/
*.pyc
.venv
.DS_Store
tests/  # Or any other test directories
.mypy_cache/
.ruff_cache/
```

# 13. Readme file

The README.md file should be created by you and should be the last step in your response.
The file should include all the information about the project including at least the following sections:
* Project Overview
* Table of Contents
* Key Features
* Code Structure (with links to the main files and folders)
* Local Development
* Docker Development
* Production Deployment on AWS (brief description, more details in DEPLOYMENT.md)
* API Endpoints
* Code Validation (using mypy and ruff for static type checking and linting)
* Loading Initial Papers

# 14. Instructions

Now please create the whole project for me following all the provided instructions and best practices.
Make sure that your Python code follows the best practices and provided style guide, in particular that type hints are provided where needed.

No need to output file content if this file is exactly the same as provided in section 12.
However, if you need to change something in the file, please do so and output the changed file.

Also please take into connsideration the following:
* Output README.md as the last step in your response summarizing all needed information from the project. 
* Create a separate DEPLOYMENT.md file with detailed instructions for deployment on how to get project up and running from scratch at AWS infrastructure assuming that AWS account is already created.
* README.md should include brief description of the project deployment (in addition to detailed description of everything else) and a link to DEPLOYMENT.md.